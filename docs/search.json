[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Varun Bhakhri",
    "section": "",
    "text": "I’m a grad student at UPenn studing transportation planning. I can design, plan and code. Here are my projects in geospatial analysis, transit planning and beyond.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Varun Bhakhri",
    "section": "",
    "text": "I’m a grad student at UPenn studing transportation planning. I can design, plan and code. Here are my projects in geospatial analysis, transit planning and beyond.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "Analysis\nThis section includes examples of technical analysis done using Jupyter notebooks. Each sub-section highlights different types of analyses and visualizations. In particular, it highlights that we can easily publish interactive visualizations produced with packages such as hvPlot, altair, or Folium, without losing any of the interactive features.\nOn this page, you might want to share more introductory or background information about the analyses to help guide the reader.",
    "crumbs": [
      "Analysis"
    ]
  },
  {
    "objectID": "analysis/3-altair-hvplot.html",
    "href": "analysis/3-altair-hvplot.html",
    "title": "Altair and Hvplot Charts",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive charts produced using Altair and hvPlot.",
    "crumbs": [
      "Analysis",
      "Altair and Hvplot Charts"
    ]
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in Altair",
    "text": "Example: Measles Incidence in Altair\nFirst, let’s load the data for measles incidence in wide format:\n\n\nCode\nurl = \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/measles_incidence.csv\"\ndata = pd.read_csv(url, skiprows=2, na_values=\"-\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nALABAMA\nALASKA\nARIZONA\nARKANSAS\nCALIFORNIA\nCOLORADO\nCONNECTICUT\nDELAWARE\n...\nSOUTH DAKOTA\nTENNESSEE\nTEXAS\nUTAH\nVERMONT\nVIRGINIA\nWASHINGTON\nWEST VIRGINIA\nWISCONSIN\nWYOMING\n\n\n\n\n0\n1928\n1\n3.67\nNaN\n1.90\n4.11\n1.38\n8.38\n4.50\n8.58\n...\n5.69\n22.03\n1.18\n0.4\n0.28\nNaN\n14.83\n3.36\n1.54\n0.91\n\n\n1\n1928\n2\n6.25\nNaN\n6.40\n9.91\n1.80\n6.02\n9.00\n7.30\n...\n6.57\n16.96\n0.63\nNaN\n0.56\nNaN\n17.34\n4.19\n0.96\nNaN\n\n\n2\n1928\n3\n7.95\nNaN\n4.50\n11.15\n1.31\n2.86\n8.81\n15.88\n...\n2.04\n24.66\n0.62\n0.2\n1.12\nNaN\n15.67\n4.19\n4.79\n1.36\n\n\n3\n1928\n4\n12.58\nNaN\n1.90\n13.75\n1.87\n13.71\n10.40\n4.29\n...\n2.19\n18.86\n0.37\n0.2\n6.70\nNaN\n12.77\n4.66\n1.64\n3.64\n\n\n4\n1928\n5\n8.03\nNaN\n0.47\n20.79\n2.38\n5.13\n16.80\n5.58\n...\n3.94\n20.05\n1.57\n0.4\n6.70\nNaN\n18.83\n7.37\n2.91\n0.91\n\n\n\n\n5 rows × 53 columns\n\n\n\nThen, use the pandas.melt() function to convert it to tidy format:\n\n\nCode\nannual = data.drop(\"WEEK\", axis=1)\nmeasles = annual.groupby(\"YEAR\").sum().reset_index()\nmeasles = measles.melt(id_vars=\"YEAR\", var_name=\"state\", value_name=\"incidence\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nstate\nincidence\n\n\n\n\n0\n1928\nALABAMA\n334.99\n\n\n1\n1929\nALABAMA\n111.93\n\n\n2\n1930\nALABAMA\n157.00\n\n\n3\n1931\nALABAMA\n337.29\n\n\n4\n1932\nALABAMA\n10.21\n\n\n\n\n\n\n\nFinally, load altair:\n\nimport altair as alt\n\nAnd generate our final data viz:\n\n# use a custom color map\ncolormap = alt.Scale(\n    domain=[0, 100, 200, 300, 1000, 3000],\n    range=[\n        \"#F0F8FF\",\n        \"cornflowerblue\",\n        \"mediumseagreen\",\n        \"#FFEE00\",\n        \"darkorange\",\n        \"firebrick\",\n    ],\n    type=\"sqrt\",\n)\n\n# Vertical line for vaccination year\nthreshold = pd.DataFrame([{\"threshold\": 1963}])\n\n# plot YEAR vs state, colored by incidence\nchart = (\n    alt.Chart(measles)\n    .mark_rect()\n    .encode(\n        x=alt.X(\"YEAR:O\", axis=alt.Axis(title=None, ticks=False)),\n        y=alt.Y(\"state:N\", axis=alt.Axis(title=None, ticks=False)),\n        color=alt.Color(\"incidence:Q\", sort=\"ascending\", scale=colormap, legend=None),\n        tooltip=[\"state\", \"YEAR\", \"incidence\"],\n    )\n    .properties(width=650, height=500)\n)\n\nrule = alt.Chart(threshold).mark_rule(strokeWidth=4).encode(x=\"threshold:O\")\n\nout = chart + rule\nout",
    "crumbs": [
      "Analysis",
      "Altair and Hvplot Charts"
    ]
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in hvplot",
    "text": "Example: Measles Incidence in hvplot\n\n\n\n\n\n\n\n\n\n\n\nGenerate the same data viz in hvplot:\n\n# Make the heatmap with hvplot\nheatmap = measles.hvplot.heatmap(\n    x=\"YEAR\",\n    y=\"state\",\n    C=\"incidence\", # color each square by the incidence\n    reduce_function=np.sum, # sum the incidence for each state/year\n    frame_height=450,\n    frame_width=600,\n    flip_yaxis=True,\n    rot=90,\n    colorbar=False,\n    cmap=\"viridis\",\n    xlabel=\"\",\n    ylabel=\"\",\n)\n\n# Some additional formatting using holoviews \n# For more info: http://holoviews.org/user_guide/Customizing_Plots.html\nheatmap = heatmap.redim(state=\"State\", YEAR=\"Year\")\nheatmap = heatmap.opts(fontsize={\"xticks\": 0, \"yticks\": 6}, toolbar=\"above\")\nheatmap",
    "crumbs": [
      "Analysis",
      "Altair and Hvplot Charts"
    ]
  },
  {
    "objectID": "analysis/1-python-code-blocks.html",
    "href": "analysis/1-python-code-blocks.html",
    "title": "Python code blocks",
    "section": "",
    "text": "This is an example from the Quarto documentation that shows how to mix executable Python code blocks into a markdown file in a “Quarto markdown” .qmd file.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis",
    "crumbs": [
      "Analysis",
      "Python code blocks"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hello everyone\n\n\nOn this about page, you might want to add more information about yourself, the project, or course.\n\n\nMy name is Varun Bhakhri.\nYou can find more information about me on my personal website.\nThis site is a home for my projects in geospatial analysis for urban planning using R and Python",
    "crumbs": [
      "About Me"
    ]
  },
  {
    "objectID": "analysis/2-static-images.html",
    "href": "analysis/2-static-images.html",
    "title": "Showing static visualizations",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and demonstrates how to generate static visualizations with matplotlib, pandas, and seaborn.\nStart by importing the packages we need:\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nLoad the “Palmer penguins” dataset from week 2:\n# Load data on Palmer penguins\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/penguins.csv\")\n# Show the first ten rows\npenguins.head(n=10)    \n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nfemale\n2007\n\n\n7\nAdelie\nTorgersen\n39.2\n19.6\n195.0\n4675.0\nmale\n2007\n\n\n8\nAdelie\nTorgersen\n34.1\n18.1\n193.0\n3475.0\nNaN\n2007\n\n\n9\nAdelie\nTorgersen\n42.0\n20.2\n190.0\n4250.0\nNaN\n2007",
    "crumbs": [
      "Analysis",
      "Showing static visualizations"
    ]
  },
  {
    "objectID": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "href": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "title": "Showing static visualizations",
    "section": "A simple visualization, 3 different ways",
    "text": "A simple visualization, 3 different ways\n\nI want to scatter flipper length vs. bill length, colored by the penguin species\n\n\nUsing matplotlib\n\n# Setup a dict to hold colors for each species\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Initialize the figure \"fig\" and axes \"ax\"\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Group the data frame by species and loop over each group\n# NOTE: \"group\" will be the dataframe holding the data for \"species\"\nfor species, group_df in penguins.groupby(\"species\"):\n\n    # Plot flipper length vs bill length for this group\n    # Note: we are adding this plot to the existing \"ax\" object\n    ax.scatter(\n        group_df[\"flipper_length_mm\"],\n        group_df[\"bill_length_mm\"],\n        marker=\"o\",\n        label=species,\n        color=color_map[species],\n        alpha=0.75,\n        zorder=10\n    )\n\n# Plotting is done...format the axes!\n\n## Add a legend to the axes\nax.legend(loc=\"best\")\n\n## Add x-axis and y-axis labels\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\n\n## Add the grid of lines\nax.grid(True);\n\n\n\n\n\n\n\n\n\n\nHow about in pandas?\nDataFrames have a built-in “plot” function that can make all of the basic type of matplotlib plots!\nFirst, we need to add a new “color” column specifying the color to use for each species type.\nUse the pd.replace() function: it use a dict to replace values in a DataFrame column.\n\n# Calculate a list of colors\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Map species name to color \npenguins[\"color\"] = penguins[\"species\"].replace(color_map)\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\ncolor\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n#1f77b4\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n#1f77b4\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n#1f77b4\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n#1f77b4\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n#1f77b4\n\n\n\n\n\n\n\nNow plot!\n\n# Same as before: Start by initializing the figure and axes\nfig, myAxes = plt.subplots(figsize=(10, 6))\n\n# Scatter plot two columns, colored by third\n# Use the built-in pandas plot.scatter function\npenguins.plot.scatter(\n    x=\"flipper_length_mm\",\n    y=\"bill_length_mm\",\n    c=\"color\",\n    alpha=0.75,\n    ax=myAxes, # IMPORTANT: Make sure to plot on the axes object we created already!\n    zorder=10\n)\n\n# Format the axes finally\nmyAxes.set_xlabel(\"Flipper Length (mm)\")\nmyAxes.set_ylabel(\"Bill Length (mm)\")\nmyAxes.grid(True);\n\n\n\n\n\n\n\n\nNote: no easy way to get legend added to the plot in this case…\n\n\nSeaborn: statistical data visualization\nSeaborn is designed to plot two columns colored by a third column…\n\n# Initialize the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# style keywords as dict\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\nstyle = dict(palette=color_map, s=60, edgecolor=\"none\", alpha=0.75, zorder=10)\n\n# use the scatterplot() function\nsns.scatterplot(\n    x=\"flipper_length_mm\",  # the x column\n    y=\"bill_length_mm\",  # the y column\n    hue=\"species\",  # the third dimension (color)\n    data=penguins,  # pass in the data\n    ax=ax,  # plot on the axes object we made\n    **style  # add our style keywords\n)\n\n# Format with matplotlib commands\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\nax.grid(True)\nax.legend(loc=\"best\");",
    "crumbs": [
      "Analysis",
      "Showing static visualizations"
    ]
  },
  {
    "objectID": "analysis/4-folium.html",
    "href": "analysis/4-folium.html",
    "title": "Interactive Maps with Folium",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive maps produced using Folium.",
    "crumbs": [
      "Analysis",
      "Interactive Maps with Folium"
    ]
  },
  {
    "objectID": "analysis/4-folium.html#finding-the-shortest-route",
    "href": "analysis/4-folium.html#finding-the-shortest-route",
    "title": "Interactive Maps with Folium",
    "section": "Finding the shortest route",
    "text": "Finding the shortest route\nThis example finds the shortest route between the Art Musuem and the Liberty Bell using osmnx.\n\nimport osmnx as ox\n\nFirst, identify the lat/lng coordinates for our places of interest. Use osmnx to download the geometries for the Libery Bell and Art Museum.\n\nphilly_tourism = ox.features_from_place(\"Philadelphia, PA\", tags={\"tourism\": True})\n\n\nart_museum = philly_tourism.query(\"name == 'Philadelphia Museum of Art'\").squeeze()\n\nart_museum.geometry\n\n\n\n\n\n\n\n\n\nliberty_bell = philly_tourism.query(\"name == 'Liberty Bell'\").squeeze()\n\nliberty_bell.geometry\n\n\n\n\n\n\n\n\nNow, extract the lat and lng coordinates\nFor the Art Museum geometry, we can use the .geometry.centroid attribute to calculate the centroid of the building footprint.\n\nliberty_bell_x = liberty_bell.geometry.x\nliberty_bell_y = liberty_bell.geometry.y\n\n\nart_museum_x = art_museum.geometry.centroid.x\nart_museum_y = art_museum.geometry.centroid.y\n\nNext, use osmnx to download the street graph around Center City.\n\nG_cc = ox.graph_from_address(\n    \"City Hall, Philadelphia, USA\", dist=1500, network_type=\"drive\"\n)\n\nNext, identify the nodes in the graph closest to our points of interest.\n\n# Get the origin node (Liberty Bell)\norig_node = ox.nearest_nodes(G_cc, liberty_bell_x, liberty_bell_y)\n\n# Get the destination node (Art Musuem)\ndest_node = ox.nearest_nodes(G_cc, art_museum_x, art_museum_y)\n\nFind the shortest path, based on the distance of the edges:\n\n# Get the shortest path --&gt; just a list of node IDs\nroute = ox.shortest_path(G_cc, orig_node, dest_node, weight=\"length\")\n\nHow about an interactive version?\nosmnx has a helper function ox.utils_graph.route_to_gdf() to convert a route to a GeoDataFrame of edges.\n\nox.utils_graph.route_to_gdf(G_cc, route, weight=\"length\").explore(\n    tiles=\"cartodb positron\",\n    color=\"red\",\n)\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Analysis",
      "Interactive Maps with Folium"
    ]
  },
  {
    "objectID": "analysis/4-folium.html#examining-trash-related-311-requests",
    "href": "analysis/4-folium.html#examining-trash-related-311-requests",
    "title": "Interactive Maps with Folium",
    "section": "Examining Trash-Related 311 Requests",
    "text": "Examining Trash-Related 311 Requests\nFirst, let’s load the dataset from a CSV file and convert to a GeoDataFrame:\n\n\nCode\n# Load the data from a CSV file into a pandas DataFrame\ntrash_requests_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/trash_311_requests_2020.csv\"\n)\n\n# Remove rows with missing geometry\ntrash_requests_df = trash_requests_df.dropna(subset=[\"lat\", \"lon\"])\n\n\n# Create our GeoDataFrame with geometry column created from lon/lat\ntrash_requests = gpd.GeoDataFrame(\n    trash_requests_df,\n    geometry=gpd.points_from_xy(trash_requests_df[\"lon\"], trash_requests_df[\"lat\"]),\n    crs=\"EPSG:4326\",\n)\n\n\nLoad neighborhoods and do the spatial join to associate a neighborhood with each ticket:\n\n\nCode\n# Load the neighborhoods\nneighborhoods = gpd.read_file(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/zillow_neighborhoods.geojson\"\n)\n\n# Do the spatial join to add the \"ZillowName\" column\nrequests_with_hood = gpd.sjoin(\n    trash_requests,\n    neighborhoods.to_crs(trash_requests.crs),\n    predicate=\"within\",\n)\n\n\nLet’s explore the 311 requests in the Greenwich neighborhood of the city:\n\n# Extract out the point tickets for Greenwich\ngreenwich_tickets = requests_with_hood.query(\"ZillowName == 'Greenwich'\")\n\n\n# Get the neighborhood boundary for Greenwich\ngreenwich_geo = neighborhoods.query(\"ZillowName == 'Greenwich'\")\n\ngreenwich_geo.squeeze().geometry\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has callout blocks that you can use to emphasize content in different ways. This is a “Note” callout block. More info is available on the Quarto documentation.\n\n\nImport the packages we need:\n\nimport folium\nimport xyzservices\n\nCombine the tickets as markers and the neighborhood boundary on the same Folium map:\n\n# Plot the neighborhood boundary\nm = greenwich_geo.explore(\n    style_kwds={\"weight\": 4, \"color\": \"black\", \"fillColor\": \"none\"},\n    name=\"Neighborhood boundary\",\n    tiles=xyzservices.providers.CartoDB.Voyager,\n)\n\n\n# Add the individual tickets as circle markers and style them\ngreenwich_tickets.explore(\n    m=m,  # Add to the existing map!\n    marker_kwds={\"radius\": 7, \"fill\": True, \"color\": \"crimson\"},\n    marker_type=\"circle_marker\", # or 'marker' or 'circle'\n    name=\"Tickets\",\n)\n\n# Hse folium to add layer control\nfolium.LayerControl().add_to(m)\n\nm  # show map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Analysis",
      "Interactive Maps with Folium"
    ]
  },
  {
    "objectID": "analysis/RS_VB_assignment-6.html",
    "href": "analysis/RS_VB_assignment-6.html",
    "title": "Assignment 6: Predictive Modeling of Housing Prices in Philadelphia",
    "section": "",
    "text": "Riya Saini & Varun Bhakhri",
    "crumbs": [
      "Analysis",
      "Assignment 6: Predictive Modeling of Housing Prices in Philadelphia"
    ]
  },
  {
    "objectID": "analysis/RS_VB_assignment-6.html#part-2-modeling-philadelphias-housing-prices-and-algorithmic-fairness",
    "href": "analysis/RS_VB_assignment-6.html#part-2-modeling-philadelphias-housing-prices-and-algorithmic-fairness",
    "title": "Assignment 6: Predictive Modeling of Housing Prices in Philadelphia",
    "section": "Part 2: Modeling Philadelphia’s Housing Prices and Algorithmic Fairness",
    "text": "Part 2: Modeling Philadelphia’s Housing Prices and Algorithmic Fairness\n\n2.1 Load data from the Office of Property Assessment\nUse the requests package to query the CARTO API for single-family property assessment data in Philadelphia for properties that had their last sale during 2022.\nSources: - OpenDataPhilly - Metadata\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nimport requests\nimport hvplot.pandas\n\nnp.random.seed(42)\n\n\n\n\n\n\n\n\n\n\n\n# the CARTO API url\ncarto_url = \"https://phl.carto.com/api/v2/sql\"\n\n# Only pull 2022 sales for single family residential properties\nwhere = \"sale_date &gt;= '2022-01-01' and sale_date &lt;= '2022-12-31'\"\nwhere = where + \" and category_code_description IN ('SINGLE FAMILY', 'Single Family')\"\n\n# Create the query\nquery = f\"SELECT * FROM opa_properties_public WHERE {where}\"\n\n# Make the request\nparams = {\"q\": query, \"format\": \"geojson\"}\nresponse = requests.get(carto_url, params=params)\n\n\n# Make the GeoDataFrame\nsalesRaw = gpd.GeoDataFrame.from_features(response.json(), crs=\"EPSG:4326\")\n\n# Optional: put it a reproducible order for test/training splits later\nsalesRaw = salesRaw.sort_values(\"parcel_number\")\n\n\n# Selecting columns:\ncols = [\n    \"sale_price\",\n    \"total_livable_area\",\n    \"total_area\",\n    \"garage_spaces\",\n    \"fireplaces\",\n    \"number_of_bathrooms\",\n    \"number_of_bedrooms\",\n    \"number_stories\",\n    \"exterior_condition\",\n    \"zip_code\",\n]\n\n# Trim to these columns and remove NaNs\nsales = salesRaw[cols + [\"geometry\"]].dropna()\n\n# Trim zip code to only the first five digits\nsales[\"zip_code\"] = sales[\"zip_code\"].astype(str).str.slice(0, 5)\n\n\nsales.head()\n\n\n\n\n\n\n\n\nsale_price\ntotal_livable_area\ntotal_area\ngarage_spaces\nfireplaces\nnumber_of_bathrooms\nnumber_of_bedrooms\nnumber_stories\nexterior_condition\nzip_code\ngeometry\n\n\n\n\n1133\n450000.0\n1785.0\n1625.0\n1.0\n0.0\n2.0\n3.0\n3.0\n4\n19147\nPOINT (-75.14860 39.93145)\n\n\n13106\n670000.0\n2244.0\n1224.0\n0.0\n0.0\n3.0\n4.0\n3.0\n3\n19147\nPOINT (-75.14817 39.93101)\n\n\n7860\n790000.0\n2514.0\n1400.0\n1.0\n0.0\n0.0\n3.0\n2.0\n3\n19147\nPOINT (-75.14781 39.93010)\n\n\n835\n195000.0\n1358.0\n840.0\n0.0\n0.0\n2.0\n3.0\n3.0\n4\n19147\nPOINT (-75.14887 39.93026)\n\n\n12456\n331000.0\n868.0\n546.0\n0.0\n0.0\n2.0\n2.0\n2.0\n4\n19147\nPOINT (-75.14881 39.93012)\n\n\n\n\n\n\n\n\n\n2.2 Load data for census tracts and neighborhoods\nLoad various Philadelphia-based regions that we will use in our analysis.\n\nCensus tracts can be downloaded from: https://opendata.arcgis.com/datasets/8bc0786524a4486bb3cf0f9862ad0fbf_0.geojson\nNeighborhoods can be downloaded from: https://raw.githubusercontent.com/azavea/geo-data/master/Neighborhoods_Philadelphia/Neighborhoods_Philadelphia.geojson\n\n\ntracts = gpd.read_file(\"Data/Census_Tracts_2010.geojson\")\nneighborhoods = gpd.read_file(\"Data/philadelphia-neighborhoods.geojson\")\n\n\ntracts.head()\n\n\n\n\n\n\n\n\nOBJECTID\nSTATEFP10\nCOUNTYFP10\nTRACTCE10\nGEOID10\nNAME10\nNAMELSAD10\nMTFCC10\nFUNCSTAT10\nALAND10\nAWATER10\nINTPTLAT10\nINTPTLON10\nLOGRECNO\ngeometry\n\n\n\n\n0\n1\n42\n101\n009400\n42101009400\n94\nCensus Tract 94\nG5020\nS\n366717\n0\n+39.9632709\n-075.2322437\n10429\nPOLYGON ((-75.22927 39.96054, -75.22865 39.960...\n\n\n1\n2\n42\n101\n009500\n42101009500\n95\nCensus Tract 95\nG5020\nS\n319070\n0\n+39.9658709\n-075.2379140\n10430\nPOLYGON ((-75.23536 39.96852, -75.23545 39.969...\n\n\n2\n3\n42\n101\n009600\n42101009600\n96\nCensus Tract 96\nG5020\nS\n405273\n0\n+39.9655396\n-075.2435075\n10431\nPOLYGON ((-75.24343 39.96230, -75.24339 39.962...\n\n\n3\n4\n42\n101\n013800\n42101013800\n138\nCensus Tract 138\nG5020\nS\n341256\n0\n+39.9764504\n-075.1771771\n10468\nPOLYGON ((-75.17341 39.97779, -75.17386 39.977...\n\n\n4\n5\n42\n101\n013900\n42101013900\n139\nCensus Tract 139\nG5020\nS\n562934\n0\n+39.9750563\n-075.1711846\n10469\nPOLYGON ((-75.17313 39.97776, -75.17321 39.977...\n\n\n\n\n\n\n\n\nneighborhoods.head()\n\n\n\n\n\n\n\n\nNAME\nLISTNAME\nMAPNAME\nShape_Leng\nShape_Area\ngeometry\n\n\n\n\n0\nBRIDESBURG\nBridesburg\nBridesburg\n27814.546521\n4.458626e+07\nMULTIPOLYGON (((-75.06773 40.00540, -75.06765 ...\n\n\n1\nBUSTLETON\nBustleton\nBustleton\n48868.458365\n1.140504e+08\nMULTIPOLYGON (((-75.01560 40.09487, -75.01768 ...\n\n\n2\nCEDARBROOK\nCedarbrook\nCedarbrook\n20021.415802\n2.487174e+07\nMULTIPOLYGON (((-75.18848 40.07273, -75.18846 ...\n\n\n3\nCHESTNUT_HILL\nChestnut Hill\nChestnut Hill\n56394.297195\n7.966498e+07\nMULTIPOLYGON (((-75.21221 40.08604, -75.21210 ...\n\n\n4\nEAST_FALLS\nEast Falls\nEast Falls\n27400.776417\n4.057689e+07\nMULTIPOLYGON (((-75.18476 40.02829, -75.18426 ...\n\n\n\n\n\n\n\n\n\n2.3 Spatially join the sales data and neighborhoods/census tracts.\nPerform a spatial join, such that each sale has an associated neighborhood and census tract.\nNote: After performing the first spatial join, you will need to use the drop() function to remove the index_right column; otherwise an error will be raised on the second spatial join about duplicate columns.\n\nsales_geo1 = gpd.sjoin(\n    sales,\n    tracts,         # Polygon data\n    how=\"left\",            # Type of join: 'left', 'inner', or 'right'\n    predicate=\"intersects\" # Use \"intersects\" to join based on spatial intersection\n)\n\n\nsales_geo1 = sales_geo1.drop(columns=['index_left', 'index_right'], errors='ignore')\nneighborhoods = neighborhoods.drop(columns=['index_left', 'index_right'], errors='ignore')\n\n\nsales_geo2 = gpd.sjoin(\n    sales_geo1,\n    neighborhoods,         # Polygon data\n    how=\"left\",            # Type of join: 'left', 'inner', or 'right'\n    predicate=\"intersects\" # Use \"intersects\" to join based on spatial intersection\n)\n\n\nsales_geo2.head()\n\n\n\n\n\n\n\n\nsale_price\ntotal_livable_area\ntotal_area\ngarage_spaces\nfireplaces\nnumber_of_bathrooms\nnumber_of_bedrooms\nnumber_stories\nexterior_condition\nzip_code\n...\nAWATER10\nINTPTLAT10\nINTPTLON10\nLOGRECNO\nindex_right\nNAME\nLISTNAME\nMAPNAME\nShape_Leng\nShape_Area\n\n\n\n\n1133\n450000.0\n1785.0\n1625.0\n1.0\n0.0\n2.0\n3.0\n3.0\n4\n19147\n...\n0\n+39.9280114\n-075.1495606\n10368\n154\nPENNSPORT\nPennsport\nPennsport\n11823.233108\n6.492473e+06\n\n\n13106\n670000.0\n2244.0\n1224.0\n0.0\n0.0\n3.0\n4.0\n3.0\n3\n19147\n...\n0\n+39.9280114\n-075.1495606\n10368\n154\nPENNSPORT\nPennsport\nPennsport\n11823.233108\n6.492473e+06\n\n\n7860\n790000.0\n2514.0\n1400.0\n1.0\n0.0\n0.0\n3.0\n2.0\n3\n19147\n...\n0\n+39.9280114\n-075.1495606\n10368\n154\nPENNSPORT\nPennsport\nPennsport\n11823.233108\n6.492473e+06\n\n\n835\n195000.0\n1358.0\n840.0\n0.0\n0.0\n2.0\n3.0\n3.0\n4\n19147\n...\n0\n+39.9280114\n-075.1495606\n10368\n154\nPENNSPORT\nPennsport\nPennsport\n11823.233108\n6.492473e+06\n\n\n12456\n331000.0\n868.0\n546.0\n0.0\n0.0\n2.0\n2.0\n2.0\n4\n19147\n...\n0\n+39.9280114\n-075.1495606\n10368\n154\nPENNSPORT\nPennsport\nPennsport\n11823.233108\n6.492473e+06\n\n\n\n\n5 rows × 31 columns\n\n\n\n\n# Check columns:\n\nprint(sales_geo1.columns)\n\nprint(sales_geo2.columns)\n\nIndex(['sale_price', 'total_livable_area', 'total_area', 'garage_spaces',\n       'fireplaces', 'number_of_bathrooms', 'number_of_bedrooms',\n       'number_stories', 'exterior_condition', 'zip_code', 'geometry',\n       'OBJECTID', 'STATEFP10', 'COUNTYFP10', 'TRACTCE10', 'GEOID10', 'NAME10',\n       'NAMELSAD10', 'MTFCC10', 'FUNCSTAT10', 'ALAND10', 'AWATER10',\n       'INTPTLAT10', 'INTPTLON10', 'LOGRECNO'],\n      dtype='object')\nIndex(['sale_price', 'total_livable_area', 'total_area', 'garage_spaces',\n       'fireplaces', 'number_of_bathrooms', 'number_of_bedrooms',\n       'number_stories', 'exterior_condition', 'zip_code', 'geometry',\n       'OBJECTID', 'STATEFP10', 'COUNTYFP10', 'TRACTCE10', 'GEOID10', 'NAME10',\n       'NAMELSAD10', 'MTFCC10', 'FUNCSTAT10', 'ALAND10', 'AWATER10',\n       'INTPTLAT10', 'INTPTLON10', 'LOGRECNO', 'index_right', 'NAME',\n       'LISTNAME', 'MAPNAME', 'Shape_Leng', 'Shape_Area'],\n      dtype='object')\n\n\n\n# The feature columns we want to use:\n\ncols = [\n    \"sale_price\",\n    \"total_livable_area\",\n    \"total_area\",\n    \"garage_spaces\",\n    \"fireplaces\",\n    \"number_of_bathrooms\",\n    \"number_of_bedrooms\",\n    \"number_stories\",\n    \"exterior_condition\",\n    \"GEOID10\", \n    \"NAME\"\n]\n\n# Trim to these columns and remove NaNs\nsales = sales_geo2[cols + [\"geometry\"]].dropna()\n\n\n\n2.4 Train a Random Forest on the sales data\nIn this step, you should follow the steps outlined in lecture to preprocess and train your model. We’ll extend our analysis to do a hyperparameter grid search to find the best model configuration. As you train your model, follow the following steps:\nPreprocessing Requirements - Trim the sales data to those sales with prices between $3,000 and $1 million - Set up a pipeline that includes both numerical columns and categorical columns - Include one-hot encoded variable for the neighborhood of the sale, instead of ZIP code. We don’t want to include multiple location based categories, since they encode much of the same information.\nTraining requirements - Use a 70/30% training/test split and predict the log of the sales price. - Use GridSearchCV to perform a k-fold cross validation that optimize at least 2 hyperparameters of the RandomForestRegressor - After fitting your model and finding the optimal hyperparameters, you should evaluate the score (R-squared) on the test set (the original 30% sample withheld)\nNote: You don’t need to include additional features (such as spatial distance features) or perform any extra feature engineering beyond what is required above to receive full credit. Of course, you are always welcome to experiment!\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import r2_score\nimport numpy as np\n\n#Preprocessing:\n\n# Filter valid sales\nvalid = (sales_geo2['sale_price'] &gt; 3000) & (sales_geo2['sale_price'] &lt; 1000000)\nsales_final = sales_geo2.loc[valid]\n\n# Numerical columns\nnum_cols = [\n    \"total_livable_area\",\n    \"total_area\",\n    \"garage_spaces\",\n    \"fireplaces\",\n    \"number_of_bathrooms\",\n    \"number_of_bedrooms\",\n    \"number_stories\",\n]\n\n# Categorical columns\ncat_cols = [\"exterior_condition\", \"NAME\"]\n\n# Preprocessor\ntransformer = ColumnTransformer(\n    transformers=[\n        (\"num\", StandardScaler(), num_cols),\n        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n    ]\n)\n\n# Initialize RandomForestRegressor (no specific hyperparameters yet)\nrf = RandomForestRegressor(random_state=42)\n\n# Build pipeline\npipe = Pipeline(steps=[\n    ('preprocessor', transformer),\n    ('regressor', rf)\n])\n\n# Split data 70/30\ntrain_set, test_set = train_test_split(sales_final, test_size=0.3, random_state=42)\n\n# Separate features (X) and target (y)\nX_train = train_set.drop(columns=[\"sale_price\"])\nX_test = test_set.drop(columns=[\"sale_price\"])\ny_train = np.log(train_set[\"sale_price\"])\ny_test = np.log(test_set[\"sale_price\"])\n\n# Define hyperparameter grid for RandomForest\nparam_grid = {\n    'regressor__n_estimators': [50, 100],\n    'regressor__max_depth': [10, 20],\n    'regressor__min_samples_split': [2, 5]\n}\n\n\n# Perform GridSearchCV with k-fold cross-validation\ngrid_search = GridSearchCV(\n    pipe,\n    param_grid=param_grid,\n    cv=3,  # k-fold cross-validation\n    scoring='r2',  # R-squared as the scoring metric\n    n_jobs=-1,  # Use all processors\n    verbose=2\n)\n\n# Fit GridSearchCV\ngrid_search.fit(X_train, y_train)\n\n# Best hyperparameters\nprint(\"Best Parameters:\", grid_search.best_params_)\n\n# Evaluate the model on the test set\ny_pred = grid_search.best_estimator_.predict(X_test)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"R-squared on Test Set: {r2:.4f}\")\n\nFitting 3 folds for each of 8 candidates, totalling 24 fits\nBest Parameters: {'regressor__max_depth': 20, 'regressor__min_samples_split': 5, 'regressor__n_estimators': 100}\nR-squared on Test Set: 0.5505\n\n\n\n\n2.5 Calculate the percent error of your model predictions for each sale in the test set\nFit your best model and use it to make predictions on the test set.\nNote: This should be the percent error in terms of sale price. You’ll need to convert if your model predicted the log of sales price!\n\n# Convert log predictions back to original sale prices\ny_pred_log = grid_search.best_estimator_.predict(X_test)\ny_pred = np.exp(y_pred_log)  # Convert from log scale to original scale\n\n# Actual sale prices from the test set\ny_actual = np.exp(y_test)  # Convert log-transformed test target back to original scale\n\n# Calculate percent error for each sale\npercent_error = 100 * np.abs((y_actual - y_pred) / y_actual)\n\n# Add percent error to a DataFrame for easier inspection\ntest_results = test_set.copy()\ntest_results[\"Actual Sale Price\"] = y_actual\ntest_results[\"Predicted Sale Price\"] = y_pred\ntest_results[\"Percent Error (%)\"] = percent_error\n\n\ntest_results.head(10)\n\n\n\n\n\n\n\n\nsale_price\ntotal_livable_area\ntotal_area\ngarage_spaces\nfireplaces\nnumber_of_bathrooms\nnumber_of_bedrooms\nnumber_stories\nexterior_condition\nzip_code\n...\nLOGRECNO\nindex_right\nNAME\nLISTNAME\nMAPNAME\nShape_Leng\nShape_Area\nActual Sale Price\nPredicted Sale Price\nPercent Error (%)\n\n\n\n\n8156\n120000.0\n1284.0\n1072.0\n0.0\n0.0\n1.0\n3.0\n1.0\n4\n19143\n...\n10413\n118\nCOBBS_CREEK\nCobbs Creek\nCobbs Creek\n25355.994460\n3.755230e+07\n120000.0\n153636.386142\n28.030322\n\n\n13566\n58500.0\n1792.0\n1120.0\n0.0\n0.0\n1.0\n4.0\n2.0\n4\n19134\n...\n10509\n76\nUPPER_KENSINGTON\nUpper Kensington\nUpper Kensington\n19841.099278\n2.278303e+07\n58500.0\n83345.391195\n42.470754\n\n\n22550\n80000.0\n1120.0\n937.0\n0.0\n0.0\n2.0\n3.0\n2.0\n4\n19142\n...\n10397\n115\nPASCHALL\nPaschall\nPaschall\n22650.029678\n1.966201e+07\n80000.0\n189014.571783\n136.268215\n\n\n19208\n150000.0\n1046.0\n1209.0\n1.0\n0.0\n1.0\n3.0\n2.0\n4\n19136\n...\n10639\n39\nHOLMESBURG\nHolmesburg\nHolmesburg\n40376.277535\n8.406447e+07\n150000.0\n153004.834966\n2.003223\n\n\n19616\n157100.0\n900.0\n1396.0\n0.0\n0.0\n1.0\n2.0\n2.0\n4\n19128\n...\n10536\n50\nROXBOROUGH\nRoxborough\nRoxborough\n23054.709844\n2.241660e+07\n157100.0\n168547.045501\n7.286471\n\n\n5193\n100000.0\n896.0\n1293.0\n0.0\n0.0\n1.0\n3.0\n2.0\n4\n19124\n...\n10517\n73\nJUNIATA_PARK\nJuniata Park\nJuniata Park\n30347.187784\n3.244185e+07\n100000.0\n118884.863137\n18.884863\n\n\n18465\n284999.0\n820.0\n786.0\n0.0\n0.0\n1.0\n2.0\n2.0\n4\n19125\n...\n10488\n149\nEAST_KENSINGTON\nEast Kensington\nEast Kensington\n11570.514789\n7.672347e+06\n284999.0\n172547.643360\n39.456755\n\n\n16338\n200000.0\n1360.0\n3523.0\n0.0\n0.0\n1.0\n3.0\n2.0\n4\n19154\n...\n10683\n17\nPARKWOOD_MANOR\nParkwood Manor\nParkwood Manor\n36872.934875\n4.700604e+07\n200000.0\n267007.092470\n33.503546\n\n\n20174\n26000.0\n1182.0\n845.0\n0.0\n0.0\n1.0\n3.0\n2.0\n4\n19140\n...\n10526\n62\nTIOGA\nTioga\nTioga\n27189.667081\n3.162796e+07\n26000.0\n114379.990600\n339.923041\n\n\n17738\n17000.0\n702.0\n520.0\n0.0\n0.0\n1.0\n2.0\n1.0\n4\n19140\n...\n10521\n76\nUPPER_KENSINGTON\nUpper Kensington\nUpper Kensington\n19841.099278\n2.278303e+07\n17000.0\n42469.103986\n149.818259\n\n\n\n\n10 rows × 34 columns\n\n\n\n\n\n2.6 Make a data frame with percent errors and census tract info for each sale in the test set\nCreate a data frame that has the property geometries, census tract data, and percent errors for all of the sales in the test set.\nNotes\n\nWhen using the “train_test_split()” function, the index of the test data frame includes the labels from the original sales data frame\nYou can use this index to slice out the test data from the original sales data frame, which should include the census tract info and geometries\nAdd a new column to this data frame holding the percent error data\nMake sure to use the percent error and not the absolute percent error\n\n\ntest_data_with_geometry = sales.loc[test_set.index]\n\ntest_data_census = test_data_with_geometry.join(\n    test_results[\"Percent Error (%)\"], how=\"left\"\n)\n\ntest_data_census.head()\n\n\n\n\n\n\n\n\nsale_price\ntotal_livable_area\ntotal_area\ngarage_spaces\nfireplaces\nnumber_of_bathrooms\nnumber_of_bedrooms\nnumber_stories\nexterior_condition\nGEOID10\nNAME\ngeometry\nPercent Error (%)\n\n\n\n\n8156\n120000.0\n1284.0\n1072.0\n0.0\n0.0\n1.0\n3.0\n1.0\n4\n42101008102\nCOBBS_CREEK\nPOINT (-75.23493 39.95220)\n28.030322\n\n\n13566\n58500.0\n1792.0\n1120.0\n0.0\n0.0\n1.0\n4.0\n2.0\n4\n42101017702\nUPPER_KENSINGTON\nPOINT (-75.12162 39.99780)\n42.470754\n\n\n22550\n80000.0\n1120.0\n937.0\n0.0\n0.0\n2.0\n3.0\n2.0\n4\n42101006400\nPASCHALL\nPOINT (-75.23767 39.92786)\n136.268215\n\n\n19208\n150000.0\n1046.0\n1209.0\n1.0\n0.0\n1.0\n3.0\n2.0\n4\n42101032900\nHOLMESBURG\nPOINT (-75.02733 40.03371)\n2.003223\n\n\n19616\n157100.0\n900.0\n1396.0\n0.0\n0.0\n1.0\n2.0\n2.0\n4\n42101021000\nROXBOROUGH\nPOINT (-75.21195 40.02551)\n7.286471\n\n\n\n\n\n\n\n\n\n2.8 Plot a map of the median percent error by census tract\n\nYou’ll want to group your data frame of test sales by the GEOID10 column and take the median of you percent error column\nMerge the census tract geometries back in and use geopandas to plot.\n\n\n#Group by GEOID10 and calculate the median percent error\n\ntest_data_grouped = test_data_census.groupby(\"GEOID10\")[\"Percent Error (%)\"].median().reset_index()\ntest_data_grouped.rename(columns={\"Percent Error (%)\": \"Median Percent Error\"}, inplace=True)\n\ntest_data_tracts = tracts.merge(test_data_grouped, on=\"GEOID10\", how=\"left\")\n\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\ntest_data_tracts.plot(\n    column=\"Median Percent Error\",  # Data to color by\n    cmap=\"coolwarm\",  # Color map\n    legend=True,\n    legend_kwds={\"label\": \"Median Percent Error (%)\"},\n    ax=ax,\n    missing_kwds={\"color\": \"lightgrey\", \"label\": \"No Data\"}\n)\nax.set_title(\"Median Percent Error by Census Tract\", fontsize=16)\nax.axis(\"off\")\n\nplt.show()\n\n\n\n\n\n\n\n\nOne census tract has a high error and is skewing the choropleth\n\ntest_data_tracts[\"Error Quantile\"] = pd.qcut(\n    test_data_tracts[\"Median Percent Error\"], \n    q=4,  # Divide into 4 quantiles (quartiles)\n    labels=[\"Low\", \"Medium-Low\", \"Medium-High\", \"High\"]\n)\n\n# Step 2: Plot using GeoPandas\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\ntest_data_tracts.plot(\n    column=\"Error Quantile\",  # Data to classify by\n    cmap=\"coolwarm\",  # Color map\n    legend=True,\n    legend_kwds={\n        \"title\": \"Percent Error Quantiles\",\n        \"loc\": \"lower right\"\n    },\n    ax=ax,\n    missing_kwds={\"color\": \"lightgrey\", \"label\": \"No Data\"}\n)\nax.set_title(\"Median Percent Error by Census Tract (Classified by Quantiles)\", fontsize=16)\nax.axis(\"off\")\n\n\n\n\n\n\n\n\nClassifying the errors by quantiles reveals a clearer picture. Tracts in Central Philadelphia have a higher error rate than others.\n\n\n2.9 Compare the percent errors in Qualifying Census Tracts and other tracts\nQualifying Census Tracts are a poverty designation that HUD uses to allocate housing tax credits\n\nI’ve included a list of the census tract names that qualify in Philadelphia\nAdd a new column to your dataframe of test set sales that is True/False depending on if the tract is a QCT\nThen, group by this new column and calculate the median percent error\n\nYou should find that the algorithm’s accuracy is significantly worse in these low-income, qualifying census tracts\n\ntest_data_tracts2 = test_data_tracts.merge(\n    tracts[['GEOID10', 'NAME10']],  # Select only GEOID and NAME10 columns\n    on='GEOID10', \n    how='left'\n)\n\ntest_data_tracts2.head()\n\n\n\n\n\n\n\n\nOBJECTID\nSTATEFP10\nCOUNTYFP10\nTRACTCE10\nGEOID10\nNAME10_x\nNAMELSAD10\nMTFCC10\nFUNCSTAT10\nALAND10\nAWATER10\nINTPTLAT10\nINTPTLON10\nLOGRECNO\ngeometry\nMedian Percent Error\nError Quantile\nNAME10_y\n\n\n\n\n0\n1\n42\n101\n009400\n42101009400\n94\nCensus Tract 94\nG5020\nS\n366717\n0\n+39.9632709\n-075.2322437\n10429\nPOLYGON ((-75.22927 39.96054, -75.22865 39.960...\n52.133105\nHigh\n94\n\n\n1\n2\n42\n101\n009500\n42101009500\n95\nCensus Tract 95\nG5020\nS\n319070\n0\n+39.9658709\n-075.2379140\n10430\nPOLYGON ((-75.23536 39.96852, -75.23545 39.969...\n43.830656\nHigh\n95\n\n\n2\n3\n42\n101\n009600\n42101009600\n96\nCensus Tract 96\nG5020\nS\n405273\n0\n+39.9655396\n-075.2435075\n10431\nPOLYGON ((-75.24343 39.96230, -75.24339 39.962...\n41.781578\nHigh\n96\n\n\n3\n4\n42\n101\n013800\n42101013800\n138\nCensus Tract 138\nG5020\nS\n341256\n0\n+39.9764504\n-075.1771771\n10468\nPOLYGON ((-75.17341 39.97779, -75.17386 39.977...\n38.237322\nHigh\n138\n\n\n4\n5\n42\n101\n013900\n42101013900\n139\nCensus Tract 139\nG5020\nS\n562934\n0\n+39.9750563\n-075.1711846\n10469\nPOLYGON ((-75.17313 39.97776, -75.17321 39.977...\n7.764229\nLow\n139\n\n\n\n\n\n\n\n\nqct = ['5',\n '20',\n '22',\n '28.01',\n '30.01',\n '30.02',\n '31',\n '32',\n '33',\n '36',\n '37.01',\n '37.02',\n '39.01',\n '41.01',\n '41.02',\n '56',\n '60',\n '61',\n '62',\n '63',\n '64',\n '65',\n '66',\n '67',\n '69',\n '70',\n '71.01',\n '71.02',\n '72',\n '73',\n '74',\n '77',\n '78',\n '80',\n '81.01',\n '81.02',\n '82',\n '83.01',\n '83.02',\n '84',\n '85',\n '86.01',\n '86.02',\n '87.01',\n '87.02',\n '88.01',\n '88.02',\n '90',\n '91',\n '92',\n '93',\n '94',\n '95',\n '96',\n '98.01',\n '100',\n '101',\n '102',\n '103',\n '104',\n '105',\n '106',\n '107',\n '108',\n '109',\n '110',\n '111',\n '112',\n '113',\n '119',\n '121',\n '122.01',\n '122.03',\n '131',\n '132',\n '137',\n '138',\n '139',\n '140',\n '141',\n '144',\n '145',\n '146',\n '147',\n '148',\n '149',\n '151.01',\n '151.02',\n '152',\n '153',\n '156',\n '157',\n '161',\n '162',\n '163',\n '164',\n '165',\n '167.01',\n '167.02',\n '168',\n '169.01',\n '169.02',\n '170',\n '171',\n '172.01',\n '172.02',\n '173',\n '174',\n '175',\n '176.01',\n '176.02',\n '177.01',\n '177.02',\n '178',\n '179',\n '180.02',\n '188',\n '190',\n '191',\n '192',\n '195.01',\n '195.02',\n '197',\n '198',\n '199',\n '200',\n '201.01',\n '201.02',\n '202',\n '203',\n '204',\n '205',\n '206',\n '208',\n '239',\n '240',\n '241',\n '242',\n '243',\n '244',\n '245',\n '246',\n '247',\n '249',\n '252',\n '253',\n '265',\n '267',\n '268',\n '271',\n '274.01',\n '274.02',\n '275',\n '276',\n '277',\n '278',\n '279.01',\n '279.02',\n '280',\n '281',\n '282',\n '283',\n '284',\n '285',\n '286',\n '287',\n '288',\n '289.01',\n '289.02',\n '290',\n '291',\n '293',\n '294',\n '298',\n '299',\n '300',\n '301',\n '302',\n '305.01',\n '305.02',\n '309',\n '311.01',\n '312',\n '313',\n '314.01',\n '314.02',\n '316',\n '318',\n '319',\n '321',\n '325',\n '329',\n '330',\n '337.01',\n '345.01',\n '357.01',\n '376',\n '377',\n '380',\n '381',\n '382',\n '383',\n '389',\n '390']\n\n\ntest_data_tracts['Is_QCT'] = test_data_tracts['NAME10'].isin(qct)\n\n# Step 3: Group by the new Is_QCT column and calculate median percent error\nmedian_percent_error_by_qct = test_data_tracts.groupby('Is_QCT')['Median Percent Error'].median().reset_index()\n\n\nmedian_percent_error_by_qct.head()\n\n\n\n\n\n\n\n\nIs_QCT\nMedian Percent Error\n\n\n\n\n0\nFalse\n18.394039\n\n\n1\nTrue\n30.441475\n\n\n\n\n\n\n\nThe table shows that the error rate for predicting sales prices is significantly higher for Qualified Census Tracts than non QCTs, as expected.",
    "crumbs": [
      "Analysis",
      "Assignment 6: Predictive Modeling of Housing Prices in Philadelphia"
    ]
  }
]